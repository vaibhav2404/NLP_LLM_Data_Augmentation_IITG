# Text Augmentation for NLP

## Project Overview
This project focuses on the pivotal role of text augmentation in Natural Language Processing (NLP), particularly for low-resource languages. It employs state-of-the-art transformer-based models like BERT to enhance the adaptability and effectiveness of text classification models in scenarios where labeled data is scarce.

## Motivation
Low-resource languages suffer from a lack of extensive labeled datasets, making the development of robust NLP models challenging. This project aims to tackle this by using text augmentation to generate synthetic training data, enhancing model performance across various NLP tasks.

## Features
- Implementation of Easy Data Augmentation (EDA) techniques adapted for low-resource languages.
- Use of transformer-based models to improve the accuracy and applicability of text classification.
- Exploration of various augmentation methods such as synonym replacement, random insertion, swapping, and deletion.

